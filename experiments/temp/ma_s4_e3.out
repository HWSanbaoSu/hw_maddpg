Using good policy maddpg and adv policy ddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -21.339511071297718, agent episode reward: [-34.11622495974188, 6.388356944222078, 6.388356944222078], time: 42.453
steps: 49975, episodes: 2000, mean episode reward: -28.55987477290396, agent episode reward: [-37.864881246885936, 4.652503236990989, 4.652503236990989], time: 54.197
steps: 74975, episodes: 3000, mean episode reward: 7.177195190808797, agent episode reward: [-14.979600134351697, 11.078397662580247, 11.078397662580247], time: 54.684
steps: 99975, episodes: 4000, mean episode reward: 6.48738934098503, agent episode reward: [-13.635395471958631, 10.06139240647183, 10.06139240647183], time: 55.815
steps: 124975, episodes: 5000, mean episode reward: 5.213346435539066, agent episode reward: [-12.43529768731315, 8.824322061426107, 8.824322061426107], time: 54.715
steps: 149975, episodes: 6000, mean episode reward: 4.384657775942796, agent episode reward: [-12.195646562151556, 8.290152169047175, 8.290152169047175], time: 55.005
steps: 174975, episodes: 7000, mean episode reward: 4.364671556414725, agent episode reward: [-12.341361796619683, 8.353016676517205, 8.353016676517205], time: 55.242
steps: 199975, episodes: 8000, mean episode reward: 3.5238029368557, agent episode reward: [-13.122758083652508, 8.323280510254103, 8.323280510254103], time: 55.842
steps: 224975, episodes: 9000, mean episode reward: 3.0896774126211484, agent episode reward: [-12.861179440093023, 7.9754284263570865, 7.9754284263570865], time: 54.472
steps: 249975, episodes: 10000, mean episode reward: 2.264489419657337, agent episode reward: [-13.045188708742284, 7.654839064199811, 7.654839064199811], time: 55.613
steps: 274975, episodes: 11000, mean episode reward: 2.5341298146102136, agent episode reward: [-13.982075168582574, 8.258102491596393, 8.258102491596393], time: 55.496
steps: 299975, episodes: 12000, mean episode reward: 1.987080941792013, agent episode reward: [-14.70015060212494, 8.343615771958477, 8.343615771958477], time: 54.765
steps: 324975, episodes: 13000, mean episode reward: 1.7820767322014852, agent episode reward: [-13.573386480802638, 7.677731606502061, 7.677731606502061], time: 55.905
steps: 349975, episodes: 14000, mean episode reward: 1.8378551611389597, agent episode reward: [-13.722163027614169, 7.780009094376564, 7.780009094376564], time: 55.069
steps: 374975, episodes: 15000, mean episode reward: 1.5030964807069882, agent episode reward: [-14.810015718543516, 8.156556099625254, 8.156556099625254], time: 55.832
steps: 399975, episodes: 16000, mean episode reward: 1.1931570814125518, agent episode reward: [-13.82764299246515, 7.510400036938851, 7.510400036938851], time: 56.282
steps: 424975, episodes: 17000, mean episode reward: 2.1578162350351593, agent episode reward: [-12.942451603102505, 7.550133919068832, 7.550133919068832], time: 56.209
steps: 449975, episodes: 18000, mean episode reward: 3.503717641641035, agent episode reward: [-13.935485201133753, 8.719601421387395, 8.719601421387395], time: 55.75
steps: 474975, episodes: 19000, mean episode reward: 2.847416197233036, agent episode reward: [-12.724182007811255, 7.7857991025221445, 7.7857991025221445], time: 55.689
steps: 499975, episodes: 20000, mean episode reward: 2.8511749803465927, agent episode reward: [-13.47935382052858, 8.165264400437588, 8.165264400437588], time: 56.462
steps: 524975, episodes: 21000, mean episode reward: 2.8120318229680086, agent episode reward: [-13.085425175345877, 7.948728499156943, 7.948728499156943], time: 56.54
steps: 549975, episodes: 22000, mean episode reward: 2.6816104362015243, agent episode reward: [-13.972930172596993, 8.327270304399258, 8.327270304399258], time: 54.934
steps: 574975, episodes: 23000, mean episode reward: 3.0131477258232042, agent episode reward: [-13.701050689138782, 8.357099207480992, 8.357099207480992], time: 54.659
steps: 599975, episodes: 24000, mean episode reward: 2.978696940716949, agent episode reward: [-13.324070477951373, 8.15138370933416, 8.15138370933416], time: 55.332
steps: 624975, episodes: 25000, mean episode reward: 3.356768857952471, agent episode reward: [-13.961675053465624, 8.659221955709047, 8.659221955709047], time: 55.355
...Finished total of 25001 episodes.
