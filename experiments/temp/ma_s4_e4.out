Using good policy maddpg and adv policy ddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -22.761159401561045, agent episode reward: [-35.57331121901724, 6.406075908728099, 6.406075908728099], time: 43.451
steps: 49975, episodes: 2000, mean episode reward: -23.142999644210825, agent episode reward: [-30.659413869686922, 3.758207112738048, 3.758207112738048], time: 54.985
steps: 74975, episodes: 3000, mean episode reward: 6.659594943785085, agent episode reward: [-13.688250068266749, 10.173922506025916, 10.173922506025916], time: 55.129
steps: 99975, episodes: 4000, mean episode reward: 7.080790457366399, agent episode reward: [-12.308035121430343, 9.694412789398372, 9.694412789398372], time: 55.612
steps: 124975, episodes: 5000, mean episode reward: 5.3174148377487285, agent episode reward: [-12.296297325019612, 8.80685608138417, 8.80685608138417], time: 54.663
steps: 149975, episodes: 6000, mean episode reward: 5.047183790580441, agent episode reward: [-12.318839168017359, 8.683011479298901, 8.683011479298901], time: 55.362
steps: 174975, episodes: 7000, mean episode reward: 4.317028532183741, agent episode reward: [-13.165168420263154, 8.741098476223446, 8.741098476223446], time: 55.372
steps: 199975, episodes: 8000, mean episode reward: 4.030019686366962, agent episode reward: [-13.117404343498746, 8.573712014932854, 8.573712014932854], time: 56.02
steps: 224975, episodes: 9000, mean episode reward: 3.8616778129931912, agent episode reward: [-13.90413944253595, 8.882908627764571, 8.882908627764571], time: 54.05
steps: 249975, episodes: 10000, mean episode reward: 3.3547375870980978, agent episode reward: [-14.06604435799521, 8.710390972546655, 8.710390972546655], time: 55.826
steps: 274975, episodes: 11000, mean episode reward: 3.6026191876019356, agent episode reward: [-14.497988966643536, 9.050304077122735, 9.050304077122735], time: 55.42
steps: 299975, episodes: 12000, mean episode reward: 2.9496200198631812, agent episode reward: [-13.358240585156661, 8.153930302509922, 8.153930302509922], time: 54.973
steps: 324975, episodes: 13000, mean episode reward: 3.5891311102848364, agent episode reward: [-13.43280630402872, 8.51096870715678, 8.51096870715678], time: 56.15
steps: 349975, episodes: 14000, mean episode reward: 3.425949244138167, agent episode reward: [-13.4513419748042, 8.438645609471182, 8.438645609471182], time: 55.877
steps: 374975, episodes: 15000, mean episode reward: 3.744248659385082, agent episode reward: [-13.534605168428065, 8.639426913906572, 8.639426913906572], time: 55.609
steps: 399975, episodes: 16000, mean episode reward: 4.012497691809093, agent episode reward: [-13.190448265113027, 8.60147297846106, 8.60147297846106], time: 55.85
steps: 424975, episodes: 17000, mean episode reward: 4.017922026411284, agent episode reward: [-12.89612071840276, 8.457021372407022, 8.457021372407022], time: 56.034
steps: 449975, episodes: 18000, mean episode reward: 3.621506410413872, agent episode reward: [-13.505125154847484, 8.563315782630676, 8.563315782630676], time: 55.701
steps: 474975, episodes: 19000, mean episode reward: 3.182638480100867, agent episode reward: [-13.651326781919654, 8.41698263101026, 8.41698263101026], time: 55.933
steps: 499975, episodes: 20000, mean episode reward: 3.141234396395505, agent episode reward: [-13.993590401778437, 8.56741239908697, 8.56741239908697], time: 56.95
steps: 524975, episodes: 21000, mean episode reward: 2.8053072218271726, agent episode reward: [-14.760503162092673, 8.782905191959923, 8.782905191959923], time: 56.345
steps: 549975, episodes: 22000, mean episode reward: 2.5244841855407922, agent episode reward: [-14.12526022026557, 8.32487220290318, 8.32487220290318], time: 55.318
steps: 574975, episodes: 23000, mean episode reward: 2.3845491538491257, agent episode reward: [-14.85297554029708, 8.618762347073101, 8.618762347073101], time: 55.509
steps: 599975, episodes: 24000, mean episode reward: 3.262570496290563, agent episode reward: [-14.56207540498883, 8.912322950639696, 8.912322950639696], time: 55.723
steps: 624975, episodes: 25000, mean episode reward: 3.0441147270820132, agent episode reward: [-14.1132473608541, 8.578681043968057, 8.578681043968057], time: 53.935
...Finished total of 25001 episodes.
