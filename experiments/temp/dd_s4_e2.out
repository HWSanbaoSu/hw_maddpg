Using good policy ddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -20.793900930524227, agent episode reward: [-33.70355434366654, 6.454826706571154, 6.454826706571154], time: 43.58
steps: 49975, episodes: 2000, mean episode reward: -14.80990753248618, agent episode reward: [-16.517687939378423, 0.8538902034461184, 0.8538902034461184], time: 54.177
steps: 74975, episodes: 3000, mean episode reward: 2.679369306659893, agent episode reward: [-9.292231827138211, 5.985800566899052, 5.985800566899052], time: 56.001
steps: 99975, episodes: 4000, mean episode reward: 0.899279993910947, agent episode reward: [-8.1555131628227, 4.527396578366824, 4.527396578366824], time: 54.919
steps: 124975, episodes: 5000, mean episode reward: 0.8786867803509517, agent episode reward: [-8.5232384486431, 4.700962614497026, 4.700962614497026], time: 55.1
steps: 149975, episodes: 6000, mean episode reward: 0.33973086403134745, agent episode reward: [-8.80826586551816, 4.573998364774754, 4.573998364774754], time: 55.536
steps: 174975, episodes: 7000, mean episode reward: -0.393396591894863, agent episode reward: [-9.855734170477204, 4.7311687892911705, 4.7311687892911705], time: 54.801
steps: 199975, episodes: 8000, mean episode reward: -0.14932585958692307, agent episode reward: [-9.825118066617978, 4.837896103515528, 4.837896103515528], time: 54.642
steps: 224975, episodes: 9000, mean episode reward: -0.3175732429603204, agent episode reward: [-10.353829999349669, 5.018128378194674, 5.018128378194674], time: 55.706
steps: 249975, episodes: 10000, mean episode reward: -0.05038819699399795, agent episode reward: [-9.874964768131383, 4.912288285568693, 4.912288285568693], time: 55.523
steps: 274975, episodes: 11000, mean episode reward: -0.8860036959657636, agent episode reward: [-9.889434385874276, 4.501715344954256, 4.501715344954256], time: 54.94
steps: 299975, episodes: 12000, mean episode reward: -0.7353332748993173, agent episode reward: [-9.897012592441977, 4.580839658771331, 4.580839658771331], time: 54.415
steps: 324975, episodes: 13000, mean episode reward: -0.58666165900975, agent episode reward: [-10.120382774844636, 4.766860557917443, 4.766860557917443], time: 55.423
steps: 349975, episodes: 14000, mean episode reward: -0.12108832347732751, agent episode reward: [-10.15019397350847, 5.0145528250155715, 5.0145528250155715], time: 55.751
steps: 374975, episodes: 15000, mean episode reward: -0.19387186857793723, agent episode reward: [-9.432750233337677, 4.61943918237987, 4.61943918237987], time: 56.002
steps: 399975, episodes: 16000, mean episode reward: 0.11930531311634814, agent episode reward: [-10.044117796532667, 5.0817115548245075, 5.0817115548245075], time: 56.082
steps: 424975, episodes: 17000, mean episode reward: -0.1989550950149061, agent episode reward: [-9.524790596822635, 4.662917750903865, 4.662917750903865], time: 55.459
steps: 449975, episodes: 18000, mean episode reward: 0.15200923758954152, agent episode reward: [-9.657320086151095, 4.904664661870319, 4.904664661870319], time: 56.184
steps: 474975, episodes: 19000, mean episode reward: 0.057001086745842616, agent episode reward: [-9.711295864477117, 4.884148475611481, 4.884148475611481], time: 55.529
steps: 499975, episodes: 20000, mean episode reward: 0.27520069490232546, agent episode reward: [-10.088023182529696, 5.181611938716011, 5.181611938716011], time: 55.911
steps: 524975, episodes: 21000, mean episode reward: 0.18379912592214026, agent episode reward: [-9.73905300286163, 4.961426064391884, 4.961426064391884], time: 55.662
steps: 549975, episodes: 22000, mean episode reward: 0.12501503356276286, agent episode reward: [-9.93195279165995, 5.028483912611358, 5.028483912611358], time: 56.667
steps: 574975, episodes: 23000, mean episode reward: 0.17202199749533242, agent episode reward: [-9.738651839139022, 4.955336918317178, 4.955336918317178], time: 56.448
steps: 599975, episodes: 24000, mean episode reward: 0.34127644433989235, agent episode reward: [-10.454041060966945, 5.3976587526534185, 5.3976587526534185], time: 55.948
steps: 624975, episodes: 25000, mean episode reward: -0.2468198433555211, agent episode reward: [-10.002493663110851, 4.877836909877664, 4.877836909877664], time: 53.605
...Finished total of 25001 episodes.
