Using good policy maddpg and adv policy ddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.186190038373287, agent episode reward: [-38.2360814566801, 7.524945709153406, 7.524945709153406], time: 43.981
steps: 49975, episodes: 2000, mean episode reward: -23.06214687170131, agent episode reward: [-42.895038085840675, 9.91644560706968, 9.91644560706968], time: 54.68
steps: 74975, episodes: 3000, mean episode reward: 6.721908710897082, agent episode reward: [-16.634380410322404, 11.67814456060974, 11.67814456060974], time: 54.593
steps: 99975, episodes: 4000, mean episode reward: 7.298369632801415, agent episode reward: [-12.755367962437903, 10.02686879761966, 10.02686879761966], time: 55.666
steps: 124975, episodes: 5000, mean episode reward: 6.245664746679574, agent episode reward: [-12.271447989097021, 9.258556367888298, 9.258556367888298], time: 56.553
steps: 149975, episodes: 6000, mean episode reward: 5.9799004281673955, agent episode reward: [-12.203572616534606, 9.091736522351002, 9.091736522351002], time: 55.061
steps: 174975, episodes: 7000, mean episode reward: 4.2992560238717346, agent episode reward: [-13.003422474973654, 8.651339249422694, 8.651339249422694], time: 56.148
steps: 199975, episodes: 8000, mean episode reward: 4.48683777342748, agent episode reward: [-12.823606341709256, 8.655222057568368, 8.655222057568368], time: 55.783
steps: 224975, episodes: 9000, mean episode reward: 3.9433396323587986, agent episode reward: [-14.524988184660634, 9.234163908509716, 9.234163908509716], time: 55.001
steps: 249975, episodes: 10000, mean episode reward: 3.5550695297290527, agent episode reward: [-14.194515011334369, 8.87479227053171, 8.87479227053171], time: 54.746
steps: 274975, episodes: 11000, mean episode reward: 3.9604992680634585, agent episode reward: [-14.467300525721997, 9.213899896892729, 9.213899896892729], time: 55.561
steps: 299975, episodes: 12000, mean episode reward: 4.2835275859545865, agent episode reward: [-14.79539724534886, 9.539462415651723, 9.539462415651723], time: 56.801
steps: 324975, episodes: 13000, mean episode reward: 3.9679581093574288, agent episode reward: [-14.778461965197641, 9.373210037277534, 9.373210037277534], time: 56.079
steps: 349975, episodes: 14000, mean episode reward: 3.4570811441964735, agent episode reward: [-14.960244377144233, 9.208662760670355, 9.208662760670355], time: 54.995
steps: 374975, episodes: 15000, mean episode reward: 2.638978702698151, agent episode reward: [-14.022289419553875, 8.330634061126013, 8.330634061126013], time: 55.697
steps: 399975, episodes: 16000, mean episode reward: 2.692948483403268, agent episode reward: [-13.808876391766615, 8.25091243758494, 8.25091243758494], time: 56.862
steps: 424975, episodes: 17000, mean episode reward: 2.27218166999758, agent episode reward: [-14.23471669543963, 8.253449182718605, 8.253449182718605], time: 56.01
steps: 449975, episodes: 18000, mean episode reward: 2.729269497795815, agent episode reward: [-13.942018514823696, 8.335644006309757, 8.335644006309757], time: 55.53
steps: 474975, episodes: 19000, mean episode reward: 2.4169745363795987, agent episode reward: [-13.671517694279466, 8.044246115329532, 8.044246115329532], time: 56.775
steps: 499975, episodes: 20000, mean episode reward: 3.149973669801789, agent episode reward: [-14.274572396146276, 8.712273032974032, 8.712273032974032], time: 56.601
steps: 524975, episodes: 21000, mean episode reward: 2.4106086265487194, agent episode reward: [-14.24963297955432, 8.330120803051521, 8.330120803051521], time: 56.674
steps: 549975, episodes: 22000, mean episode reward: 2.597600745472258, agent episode reward: [-14.37044328156677, 8.484022013519514, 8.484022013519514], time: 57.229
steps: 574975, episodes: 23000, mean episode reward: 2.5080869174897527, agent episode reward: [-13.366199222935549, 7.937143070212649, 7.937143070212649], time: 56.649
steps: 599975, episodes: 24000, mean episode reward: 2.246748856918721, agent episode reward: [-13.405033477048404, 7.825891166983563, 7.825891166983563], time: 56.412
steps: 624975, episodes: 25000, mean episode reward: 2.4380976868324606, agent episode reward: [-13.841609732602262, 8.139853709717363, 8.139853709717363], time: 56.7
...Finished total of 25001 episodes.
