Using good policy ddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.345109396008034, agent episode reward: [-41.74758406007041, 9.201237332031189, 9.201237332031189], time: 41.724
steps: 49975, episodes: 2000, mean episode reward: -15.745809397428806, agent episode reward: [-22.445262771062925, 3.349726686817061, 3.349726686817061], time: 55.347
steps: 74975, episodes: 3000, mean episode reward: 3.2884719189067684, agent episode reward: [-10.30818836510912, 6.798330142007944, 6.798330142007944], time: 55.449
steps: 99975, episodes: 4000, mean episode reward: 1.794326993533539, agent episode reward: [-8.659251801811587, 5.2267893976725635, 5.2267893976725635], time: 55.84
steps: 124975, episodes: 5000, mean episode reward: 1.1431514442310613, agent episode reward: [-9.152341671919276, 5.147746558075168, 5.147746558075168], time: 55.208
steps: 149975, episodes: 6000, mean episode reward: 0.8718701683256301, agent episode reward: [-9.434013221786039, 5.152941695055835, 5.152941695055835], time: 55.061
steps: 174975, episodes: 7000, mean episode reward: -0.39164212019109734, agent episode reward: [-10.259613892495265, 4.933985886152084, 4.933985886152084], time: 56.107
steps: 199975, episodes: 8000, mean episode reward: 0.31457128065978796, agent episode reward: [-10.343008833955903, 5.328790057307845, 5.328790057307845], time: 55.806
steps: 224975, episodes: 9000, mean episode reward: 0.041633784679124985, agent episode reward: [-9.830878278959235, 4.936256031819181, 4.936256031819181], time: 55.955
steps: 249975, episodes: 10000, mean episode reward: -0.37196814615533763, agent episode reward: [-9.746349607089089, 4.687190730466875, 4.687190730466875], time: 56.556
steps: 274975, episodes: 11000, mean episode reward: -1.2763909164346978, agent episode reward: [-10.066867752714808, 4.395238418140056, 4.395238418140056], time: 54.399
steps: 299975, episodes: 12000, mean episode reward: -1.3867176899305158, agent episode reward: [-9.89617757906007, 4.254729944564778, 4.254729944564778], time: 55.722
steps: 324975, episodes: 13000, mean episode reward: -0.7826129125557518, agent episode reward: [-10.086437695953332, 4.65191239169879, 4.65191239169879], time: 56.881
steps: 349975, episodes: 14000, mean episode reward: -0.9903243830231376, agent episode reward: [-9.953391009787682, 4.481533313382271, 4.481533313382271], time: 56.633
steps: 374975, episodes: 15000, mean episode reward: -1.6085521836222545, agent episode reward: [-9.897187897758855, 4.1443178570683, 4.1443178570683], time: 55.057
steps: 399975, episodes: 16000, mean episode reward: -1.3257445857794197, agent episode reward: [-9.605157579164905, 4.139706496692742, 4.139706496692742], time: 55.924
steps: 424975, episodes: 17000, mean episode reward: -1.8250905532731756, agent episode reward: [-10.095906860439136, 4.13540815358298, 4.13540815358298], time: 55.258
steps: 449975, episodes: 18000, mean episode reward: -1.1230933984999791, agent episode reward: [-10.521498841318822, 4.699202721409422, 4.699202721409422], time: 55.577
steps: 474975, episodes: 19000, mean episode reward: -1.011557176689149, agent episode reward: [-10.596759391644717, 4.7926011074777835, 4.7926011074777835], time: 56.61
steps: 499975, episodes: 20000, mean episode reward: -1.5433522147364225, agent episode reward: [-10.725058658098629, 4.590853221681104, 4.590853221681104], time: 55.801
steps: 524975, episodes: 21000, mean episode reward: -2.0729405851497704, agent episode reward: [-9.907526323500253, 3.9172928691752418, 3.9172928691752418], time: 55.417
steps: 549975, episodes: 22000, mean episode reward: -1.9749531059581966, agent episode reward: [-10.510719625267102, 4.267883259654453, 4.267883259654453], time: 55.234
steps: 574975, episodes: 23000, mean episode reward: -1.3196131867512868, agent episode reward: [-9.89390192317517, 4.287144368211941, 4.287144368211941], time: 56.527
steps: 599975, episodes: 24000, mean episode reward: -2.3809136073612076, agent episode reward: [-10.280412461885932, 3.9497494272623617, 3.9497494272623617], time: 54.821
steps: 624975, episodes: 25000, mean episode reward: -2.035006665567237, agent episode reward: [-10.937856750477161, 4.451425042454963, 4.451425042454963], time: 55.863
...Finished total of 25001 episodes.
