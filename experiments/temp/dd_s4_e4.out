Using good policy ddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.22385645884228, agent episode reward: [-34.5331799497989, 5.6546617454783075, 5.6546617454783075], time: 43.71
steps: 49975, episodes: 2000, mean episode reward: -24.253978299861934, agent episode reward: [-40.0843246975182, 7.915173198828132, 7.915173198828132], time: 55.965
steps: 74975, episodes: 3000, mean episode reward: 2.7884137546896546, agent episode reward: [-10.110735013143811, 6.449574383916732, 6.449574383916732], time: 55.788
steps: 99975, episodes: 4000, mean episode reward: 1.8922577589670255, agent episode reward: [-9.10761377373858, 5.499935766352802, 5.499935766352802], time: 56.036
steps: 124975, episodes: 5000, mean episode reward: 1.4454491857052882, agent episode reward: [-8.840665808992279, 5.143057497348783, 5.143057497348783], time: 55.373
steps: 149975, episodes: 6000, mean episode reward: 1.2933181121073392, agent episode reward: [-9.650041984090713, 5.471680048099026, 5.471680048099026], time: 54.779
steps: 174975, episodes: 7000, mean episode reward: 0.7238111100977476, agent episode reward: [-9.318870344995098, 5.021340727546422, 5.021340727546422], time: 55.37
steps: 199975, episodes: 8000, mean episode reward: 0.0019134547101164116, agent episode reward: [-9.965771475452375, 4.9838424650812465, 4.9838424650812465], time: 55.936
steps: 224975, episodes: 9000, mean episode reward: -1.0864056612955244, agent episode reward: [-10.421454783060144, 4.66752456088231, 4.66752456088231], time: 56.155
steps: 249975, episodes: 10000, mean episode reward: -0.8291366757827356, agent episode reward: [-10.258714120566077, 4.71478872239167, 4.71478872239167], time: 56.183
steps: 274975, episodes: 11000, mean episode reward: -1.3653887483180565, agent episode reward: [-9.934821277471231, 4.284716264576588, 4.284716264576588], time: 54.41
steps: 299975, episodes: 12000, mean episode reward: -1.5539735451984698, agent episode reward: [-9.669010309813807, 4.057518382307668, 4.057518382307668], time: 55.683
steps: 324975, episodes: 13000, mean episode reward: -1.5173225830303438, agent episode reward: [-10.61332633686891, 4.548001876919283, 4.548001876919283], time: 56.927
steps: 349975, episodes: 14000, mean episode reward: -2.2132992497943036, agent episode reward: [-10.121913178425192, 3.954306964315445, 3.954306964315445], time: 56.82
steps: 374975, episodes: 15000, mean episode reward: -1.794887495039433, agent episode reward: [-10.340365556700661, 4.272739030830613, 4.272739030830613], time: 54.849
steps: 399975, episodes: 16000, mean episode reward: -1.8910854494555758, agent episode reward: [-10.079464297644577, 4.0941894240945, 4.0941894240945], time: 55.565
steps: 424975, episodes: 17000, mean episode reward: -2.388216773962793, agent episode reward: [-10.569226482148505, 4.090504854092855, 4.090504854092855], time: 55.731
steps: 449975, episodes: 18000, mean episode reward: -1.9226370834565545, agent episode reward: [-9.772924968974728, 3.925143942759087, 3.925143942759087], time: 55.476
steps: 474975, episodes: 19000, mean episode reward: -3.527407281288698, agent episode reward: [-10.420250664009238, 3.44642169136027, 3.44642169136027], time: 56.277
steps: 499975, episodes: 20000, mean episode reward: -2.54082863334644, agent episode reward: [-10.121803166908677, 3.7904872667811187, 3.7904872667811187], time: 55.589
steps: 524975, episodes: 21000, mean episode reward: -2.1194716487176075, agent episode reward: [-10.259380814884588, 4.06995458308349, 4.06995458308349], time: 55.77
steps: 549975, episodes: 22000, mean episode reward: -2.82858188475708, agent episode reward: [-9.71566496942427, 3.443541542333594, 3.443541542333594], time: 55.379
steps: 574975, episodes: 23000, mean episode reward: -3.352014557846673, agent episode reward: [-10.117595502830497, 3.3827904724919113, 3.3827904724919113], time: 56.049
steps: 599975, episodes: 24000, mean episode reward: -3.24603587251145, agent episode reward: [-10.330815739047313, 3.5423899332679323, 3.5423899332679323], time: 55.679
steps: 624975, episodes: 25000, mean episode reward: -2.625576333120721, agent episode reward: [-10.264694382261006, 3.8195590245701414, 3.8195590245701414], time: 54.857
...Finished total of 25001 episodes.
