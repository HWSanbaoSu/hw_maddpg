Using good policy maddpg and adv policy ddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.814396281431748, agent episode reward: [-40.23367754748666, 8.209640633027455, 8.209640633027455], time: 42.758
steps: 49975, episodes: 2000, mean episode reward: -19.935731764629164, agent episode reward: [-33.02177787221197, 6.543023053791401, 6.543023053791401], time: 55.9
steps: 74975, episodes: 3000, mean episode reward: 7.784708301579254, agent episode reward: [-14.606425557467345, 11.195566929523299, 11.195566929523299], time: 54.562
steps: 99975, episodes: 4000, mean episode reward: 6.570493818449479, agent episode reward: [-12.83285175875379, 9.701672788601634, 9.701672788601634], time: 55.435
steps: 124975, episodes: 5000, mean episode reward: 5.114524769251884, agent episode reward: [-11.657738307405515, 8.386131538328698, 8.386131538328698], time: 56.823
steps: 149975, episodes: 6000, mean episode reward: 4.181380245941084, agent episode reward: [-12.337531235662537, 8.259455740801812, 8.259455740801812], time: 55.684
steps: 174975, episodes: 7000, mean episode reward: 4.163582535983475, agent episode reward: [-12.360543551454626, 8.262063043719053, 8.262063043719053], time: 56.356
steps: 199975, episodes: 8000, mean episode reward: 4.361969439120424, agent episode reward: [-12.809231949786808, 8.585600694453616, 8.585600694453616], time: 56.152
steps: 224975, episodes: 9000, mean episode reward: 3.3050107848667456, agent episode reward: [-13.513142390902466, 8.409076587884607, 8.409076587884607], time: 55.52
steps: 249975, episodes: 10000, mean episode reward: 3.231181501945986, agent episode reward: [-14.182121822055853, 8.70665166200092, 8.70665166200092], time: 54.753
steps: 274975, episodes: 11000, mean episode reward: 2.6786326537614396, agent episode reward: [-14.309729998652546, 8.494181326206991, 8.494181326206991], time: 55.652
steps: 299975, episodes: 12000, mean episode reward: 2.940704111700281, agent episode reward: [-14.157517120625528, 8.549110616162904, 8.549110616162904], time: 56.807
steps: 324975, episodes: 13000, mean episode reward: 3.2117678680695754, agent episode reward: [-13.033052959637459, 8.122410413853517, 8.122410413853517], time: 55.973
steps: 349975, episodes: 14000, mean episode reward: 3.5186205354509132, agent episode reward: [-14.18925685315494, 8.853938694302927, 8.853938694302927], time: 55.769
steps: 374975, episodes: 15000, mean episode reward: 3.4606518357689255, agent episode reward: [-13.833139091169924, 8.646895463469424, 8.646895463469424], time: 56.144
steps: 399975, episodes: 16000, mean episode reward: 3.5419587631485765, agent episode reward: [-12.90880728679489, 8.225383024971734, 8.225383024971734], time: 56.904
steps: 424975, episodes: 17000, mean episode reward: 2.4615252156133747, agent episode reward: [-12.965938373801146, 7.713731794707259, 7.713731794707259], time: 56.702
steps: 449975, episodes: 18000, mean episode reward: 2.9307663444091268, agent episode reward: [-13.671681843028425, 8.301224093718776, 8.301224093718776], time: 56.493
steps: 474975, episodes: 19000, mean episode reward: 2.3692002516637314, agent episode reward: [-13.445186527978057, 7.907193389820895, 7.907193389820895], time: 56.549
steps: 499975, episodes: 20000, mean episode reward: 2.6735052035258886, agent episode reward: [-13.643751122110263, 8.158628162818076, 8.158628162818076], time: 56.259
steps: 524975, episodes: 21000, mean episode reward: 2.0090992623682253, agent episode reward: [-12.873842245087886, 7.441470753728055, 7.441470753728055], time: 56.815
steps: 549975, episodes: 22000, mean episode reward: 2.0064849435530494, agent episode reward: [-13.826410594026694, 7.9164477687898716, 7.9164477687898716], time: 57.339
steps: 574975, episodes: 23000, mean episode reward: 1.9226991399720605, agent episode reward: [-14.11731775155575, 8.020008445763906, 8.020008445763906], time: 56.956
steps: 599975, episodes: 24000, mean episode reward: 2.097824273200173, agent episode reward: [-14.155079292104908, 8.126451782652541, 8.126451782652541], time: 56.239
steps: 624975, episodes: 25000, mean episode reward: 1.8430701124277604, agent episode reward: [-13.836525970249093, 7.839798041338427, 7.839798041338427], time: 55.772
...Finished total of 25001 episodes.
