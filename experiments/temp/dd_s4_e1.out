Using good policy ddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -22.930157834509487, agent episode reward: [-37.85125236675846, 7.4605472661244905, 7.4605472661244905], time: 42.171
steps: 49975, episodes: 2000, mean episode reward: -24.00832481295233, agent episode reward: [-25.168342065203998, 0.5800086261258318, 0.5800086261258318], time: 54.701
steps: 74975, episodes: 3000, mean episode reward: 2.7506113678058184, agent episode reward: [-10.087425731203442, 6.419018549504631, 6.419018549504631], time: 54.736
steps: 99975, episodes: 4000, mean episode reward: 1.1729219041552283, agent episode reward: [-8.970261233177247, 5.071591568666237, 5.071591568666237], time: 53.811
steps: 124975, episodes: 5000, mean episode reward: 0.535908516911778, agent episode reward: [-9.162646161386757, 4.8492773391492685, 4.8492773391492685], time: 54.249
steps: 149975, episodes: 6000, mean episode reward: 0.7809107858568687, agent episode reward: [-10.24587546839851, 5.513393127127689, 5.513393127127689], time: 54.783
steps: 174975, episodes: 7000, mean episode reward: 0.6486351363165866, agent episode reward: [-10.018778095739401, 5.333706616027993, 5.333706616027993], time: 54.603
steps: 199975, episodes: 8000, mean episode reward: -0.052502747250918136, agent episode reward: [-10.16645092249561, 5.056974087622346, 5.056974087622346], time: 53.886
steps: 224975, episodes: 9000, mean episode reward: 0.09968472785745604, agent episode reward: [-9.693885924516746, 4.896785326187101, 4.896785326187101], time: 55.845
steps: 249975, episodes: 10000, mean episode reward: -0.8012309463865779, agent episode reward: [-10.377407719188511, 4.788088386400968, 4.788088386400968], time: 55.434
steps: 274975, episodes: 11000, mean episode reward: -0.5988211615331201, agent episode reward: [-9.648055718608386, 4.524617278537634, 4.524617278537634], time: 55.157
steps: 299975, episodes: 12000, mean episode reward: -1.0985658547988884, agent episode reward: [-9.819204235473569, 4.36031919033734, 4.36031919033734], time: 54.394
steps: 324975, episodes: 13000, mean episode reward: -2.043743694763992, agent episode reward: [-9.551227344086012, 3.7537418246610104, 3.7537418246610104], time: 55.636
steps: 349975, episodes: 14000, mean episode reward: -1.5874347322661986, agent episode reward: [-9.251359554398336, 3.831962411066069, 3.831962411066069], time: 55.738
steps: 374975, episodes: 15000, mean episode reward: -2.4300362283312227, agent episode reward: [-9.640898912959205, 3.605431342313991, 3.605431342313991], time: 55.932
steps: 399975, episodes: 16000, mean episode reward: -2.2906553548405837, agent episode reward: [-9.436296858308165, 3.5728207517337904, 3.5728207517337904], time: 56.205
steps: 424975, episodes: 17000, mean episode reward: -1.7363155632710348, agent episode reward: [-9.83931417690145, 4.051499306815207, 4.051499306815207], time: 56.061
steps: 449975, episodes: 18000, mean episode reward: -1.7402966793065153, agent episode reward: [-9.928639844611125, 4.094171582652305, 4.094171582652305], time: 56.34
steps: 474975, episodes: 19000, mean episode reward: -2.634356495874967, agent episode reward: [-9.977610054748158, 3.671626779436596, 3.671626779436596], time: 55.643
steps: 499975, episodes: 20000, mean episode reward: -3.1996043521224458, agent episode reward: [-10.079648270239515, 3.4400219590585337, 3.4400219590585337], time: 55.537
steps: 524975, episodes: 21000, mean episode reward: -3.503116178995403, agent episode reward: [-10.137221716819884, 3.317052768912241, 3.317052768912241], time: 55.754
steps: 549975, episodes: 22000, mean episode reward: -4.040105032358588, agent episode reward: [-10.406823692537962, 3.1833593300896874, 3.1833593300896874], time: 55.924
steps: 574975, episodes: 23000, mean episode reward: -4.211697872056378, agent episode reward: [-10.584454900424044, 3.1863785141838323, 3.1863785141838323], time: 56.598
steps: 599975, episodes: 24000, mean episode reward: -3.936913648416265, agent episode reward: [-10.095564623594314, 3.0793254875890237, 3.0793254875890237], time: 55.137
steps: 624975, episodes: 25000, mean episode reward: -5.348263883170525, agent episode reward: [-10.43831872756844, 2.5450274221989573, 2.5450274221989573], time: 55.291
...Finished total of 25001 episodes.
